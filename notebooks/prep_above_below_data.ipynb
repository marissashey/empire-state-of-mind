{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29da9a40-f8bd-409b-8e8e-1bad2b2b81a5",
   "metadata": {},
   "source": [
    "# Prep Above/Below Data Notebook\n",
    "fetch + process nyc underground (subway) + street level (citibike) data for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91cd8b3f-5218-47be-bb3e-0317191f638f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABOVE/BELOW DATA PREPARATION\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Setup and Configuration\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# create output directory\n",
    "output_dir = Path('../visualizations/above_below_data')\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(\"ABOVE/BELOW DATA PREPARATION\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff9c54c6-08f4-4fd6-91b5-3e899da99cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. FETCHING SUBWAY DATA (Underground Layer)\n",
      "----------------------------------------\n",
      "fetching subway data for 2024-01-15...\n",
      "  got 50000 records\n",
      "fetching subway data for 2024-01-16...\n",
      "  got 50000 records\n",
      "fetching subway data for 2024-01-17...\n",
      "  got 50000 records\n",
      "fetching subway data for 2024-01-18...\n",
      "  got 50000 records\n",
      "fetching subway data for 2024-01-19...\n",
      "  got 50000 records\n",
      "fetching subway data for 2024-01-20...\n",
      "  got 50000 records\n",
      "fetching subway data for 2024-01-21...\n",
      "  got 50000 records\n",
      "\n",
      "subway summary:\n",
      "  stations: 428\n",
      "  total weekly ridership: 13,487,090\n"
     ]
    }
   ],
   "source": [
    "# Fetch Subway Data (the underground)\n",
    "print(\"\\n1. FETCHING SUBWAY DATA (Underground Layer)\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# get one week of jan 2024 for prototype\n",
    "subway_url = \"https://data.ny.gov/resource/wujg-7c2s.json\"\n",
    "subway_data = []\n",
    "\n",
    "start_date = datetime(2024, 1, 15)  # monday\n",
    "end_date = datetime(2024, 1, 21)    # sunday\n",
    "\n",
    "for days in range(7):\n",
    "    date = start_date + timedelta(days=days)\n",
    "    date_str = date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    params = {\n",
    "        '$limit': 50000,\n",
    "        '$where': f\"transit_timestamp >= '{date_str}T00:00:00' AND transit_timestamp < '{date_str}T23:59:59'\"\n",
    "    }\n",
    "    \n",
    "    print(f\"fetching subway data for {date_str}...\")\n",
    "    response = requests.get(subway_url, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        subway_data.extend(data)\n",
    "        print(f\"  got {len(data)} records\")\n",
    "\n",
    "subway_df = pd.DataFrame(subway_data)\n",
    "subway_df['transit_timestamp'] = pd.to_datetime(subway_df['transit_timestamp'])\n",
    "subway_df['hour'] = subway_df['transit_timestamp'].dt.hour\n",
    "subway_df['ridership'] = pd.to_numeric(subway_df['ridership'], errors='coerce')\n",
    "\n",
    "# aggregate by station and hour\n",
    "subway_hourly = subway_df.groupby(['station_complex', 'hour', 'latitude', 'longitude']).agg({\n",
    "    'ridership': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "print(f\"\\nsubway summary:\")\n",
    "print(f\"  stations: {subway_df['station_complex'].nunique()}\")\n",
    "print(f\"  total weekly ridership: {subway_df['ridership'].sum():,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07fbaf0c-e0f4-4c46-abe4-34ecffd872d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. FETCHING CITIBIKE DATA (Street Layer)\n",
      "----------------------------------------\n",
      "fetching citibike station locations...\n",
      "  found 2240 citibike stations\n"
     ]
    }
   ],
   "source": [
    "# Fetch CitiBike Data (street level)\n",
    "print(\"\\n2. FETCHING CITIBIKE DATA (Street Layer)\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# citibike has real-time data, but also historical trips\n",
    "citibike_stations_url = \"https://gbfs.citibikenyc.com/gbfs/en/station_information.json\"\n",
    "\n",
    "print(\"fetching citibike station locations...\")\n",
    "response = requests.get(citibike_stations_url)\n",
    "citibike_stations = response.json()['data']['stations']\n",
    "print(f\"  found {len(citibike_stations)} citibike stations\")\n",
    "\n",
    "# convert to dataframe\n",
    "citibike_df = pd.DataFrame(citibike_stations)\n",
    "\n",
    "# for historical trip data, need https://s3.amazonaws.com/tripdata/202401-citibike-tripdata.csv.zip\n",
    "# but for prototype, simulating activity based on station capacity and location\n",
    "\n",
    "# create simulated hourly patterns based on station characteristics\n",
    "citibike_df['capacity'] = pd.to_numeric(citibike_df['capacity'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59b47253-9562-4899-ab18-b3a2d3cc07ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. FINDING SPATIAL RELATIONSHIPS\n",
      "----------------------------------------\n",
      "found 3556 subway-citibike connections within 500m\n"
     ]
    }
   ],
   "source": [
    "# Match Subway Stations with Nearby CitiBike Stations\n",
    "print(\"\\n3. FINDING SPATIAL RELATIONSHIPS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"calculate distance between two points in meters\"\"\"\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    return c * 6371000  # earth radius in meters\n",
    "\n",
    "# find citibike stations near each subway station\n",
    "subway_stations = subway_df[['station_complex', 'latitude', 'longitude']].drop_duplicates()\n",
    "subway_stations['latitude'] = pd.to_numeric(subway_stations['latitude'])\n",
    "subway_stations['longitude'] = pd.to_numeric(subway_stations['longitude'])\n",
    "\n",
    "proximity_threshold = 500  # meters\n",
    "\n",
    "connections = []\n",
    "for _, subway in subway_stations.iterrows():\n",
    "    nearby_bikes = []\n",
    "    for _, bike in citibike_df.iterrows():\n",
    "        distance = haversine(\n",
    "            float(subway['longitude']), float(subway['latitude']),\n",
    "            float(bike['lon']), float(bike['lat'])\n",
    "        )\n",
    "        if distance <= proximity_threshold:\n",
    "            nearby_bikes.append({\n",
    "                'subway_station': subway['station_complex'],\n",
    "                'bike_station_id': bike['station_id'],\n",
    "                'bike_station_name': bike['name'],\n",
    "                'distance': distance\n",
    "            })\n",
    "    connections.extend(nearby_bikes)\n",
    "\n",
    "connections_df = pd.DataFrame(connections)\n",
    "print(f\"found {len(connections_df)} subway-citibike connections within {proximity_threshold}m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0dcc5659-8647-4781-a665-42bb21d521e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. FETCHING WEATHER DATA\n",
      "----------------------------------------\n",
      "fetching central park weather for january 2024...\n",
      "weather summary for the week:\n",
      "  Mon 01/15: 26.9°F, precip: 0.00\"\n",
      "  Tue 01/16: 27.9°F, precip: 0.13\"\n",
      "  Wed 01/17: 22.0°F, precip: 0.19\"\n",
      "  Thu 01/18: 26.0°F, precip: 0.00\"\n",
      "  Fri 01/19: 30.6°F, precip: 0.00\"\n",
      "  Sat 01/20: 22.4°F, precip: 0.04\"\n",
      "  Sun 01/21: 23.7°F, precip: 0.00\"\n"
     ]
    }
   ],
   "source": [
    "# Fetch Weather Data (the mood modifier)\n",
    "print(\"\\n4. FETCHING WEATHER DATA\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# use noaa central park data\n",
    "weather_url = \"https://www.ncei.noaa.gov/data/global-summary-of-the-day/access/2024/72505394728.csv\"\n",
    "\n",
    "print(\"fetching central park weather for january 2024...\")\n",
    "weather_df = pd.read_csv(weather_url)\n",
    "\n",
    "# filter to week\n",
    "weather_df['DATE'] = pd.to_datetime(weather_df['DATE'])\n",
    "weather_week = weather_df[\n",
    "    (weather_df['DATE'] >= start_date) & \n",
    "    (weather_df['DATE'] <= end_date)\n",
    "].copy()\n",
    "\n",
    "# extract key weather metrics\n",
    "weather_week['temp_f'] = pd.to_numeric(weather_week['TEMP'], errors='coerce')\n",
    "weather_week['precipitation'] = pd.to_numeric(weather_week['PRCP'], errors='coerce')\n",
    "weather_week['wind_speed'] = pd.to_numeric(weather_week['WDSP'], errors='coerce')\n",
    "\n",
    "print(f\"weather summary for the week:\")\n",
    "for _, day in weather_week.iterrows():\n",
    "    print(f\"  {day['DATE'].strftime('%a %m/%d')}: {day['temp_f']:.1f}°F, precip: {day['precipitation']:.2f}\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3ef3a17-e9d9-45be-9c87-92a3b140f1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. FETCHING ART STATIONS\n",
      "----------------------------------------\n",
      "found 466 art installations at 274 stations\n",
      "\n",
      "example art stations (the portals):\n",
      "  Clark St (2,3): 1 artworks\n",
      "  125 St (A,C,B,D): 4 artworks\n",
      "  125 St (1): 4 artworks\n",
      "  125 St (4,5,6): 4 artworks\n",
      "  125 St (2,3): 4 artworks\n"
     ]
    }
   ],
   "source": [
    "# Fetch Art Stations (portals)\n",
    "print(\"\\n5. FETCHING ART STATIONS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "art_url = \"https://data.ny.gov/resource/4y8j-9pkd.json\"\n",
    "response = requests.get(art_url, params={'$limit': 1000})\n",
    "art_data = response.json()\n",
    "art_df = pd.DataFrame(art_data)\n",
    "\n",
    "# use normalized matching from earlier\n",
    "art_df['station_normalized'] = art_df['station_name'].str.lower().str.replace(r'\\([^)]*\\)', '', regex=True).str.strip()\n",
    "subway_stations['station_normalized'] = subway_stations['station_complex'].str.lower().str.replace(r'\\([^)]*\\)', '', regex=True).str.strip()\n",
    "\n",
    "# find matches\n",
    "art_matches = pd.merge(\n",
    "    art_df[['station_normalized', 'art_title', 'artist']],\n",
    "    subway_stations[['station_normalized', 'station_complex', 'latitude', 'longitude']],\n",
    "    on='station_normalized',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"found {len(art_matches)} art installations at {art_matches['station_complex'].nunique()} stations\")\n",
    "print(\"\\nexample art stations (the portals):\")\n",
    "for station in art_matches['station_complex'].unique()[:5]:\n",
    "    art_count = len(art_matches[art_matches['station_complex'] == station])\n",
    "    print(f\"  {station}: {art_count} artworks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb161f4c-3437-4154-8178-09394f43762c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. CREATING HOURLY ANIMATION DATA\n",
      "----------------------------------------\n",
      "created 168 hourly frames\n"
     ]
    }
   ],
   "source": [
    "# Create Hourly Animation Data\n",
    "print(\"\\n6. CREATING HOURLY ANIMATION DATA\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# create 168 frames (7 days × 24 hours)\n",
    "animation_data = []\n",
    "\n",
    "for day in range(7):\n",
    "    date = start_date + timedelta(days=day)\n",
    "    \n",
    "    # get weather for this day\n",
    "    day_weather = weather_week[weather_week['DATE'].dt.date == date.date()].iloc[0] if len(weather_week) > 0 else None\n",
    "    \n",
    "    for hour in range(24):\n",
    "        timestamp = date + timedelta(hours=hour)\n",
    "        \n",
    "        # subway activity for this hour\n",
    "        subway_hour = subway_hourly[subway_hourly['hour'] == hour].copy()\n",
    "        \n",
    "        # simulate citibike inverse pattern\n",
    "        # when subway is busy, bikes are less used (people underground)\n",
    "        # adjust for weather\n",
    "        if day_weather is not None and float(day_weather['precipitation']) > 0.1:  # convert to float\n",
    "            bike_multiplier = 0.3  # rain kills bike usage\n",
    "            subway_multiplier = 1.5  # rain increases subway usage\n",
    "        else:\n",
    "            bike_multiplier = 1.0\n",
    "            subway_multiplier = 1.0\n",
    "        \n",
    "        # create inverse relationship\n",
    "        if hour in [7, 8, 9, 17, 18, 19]:  # rush hours\n",
    "            bike_activity = 0.4 * bike_multiplier  # low bike use during rush\n",
    "            subway_activity = 1.0 * subway_multiplier  # high subway use\n",
    "        elif hour in [0, 1, 2, 3, 4]:  # late night\n",
    "            bike_activity = 0.1 * bike_multiplier\n",
    "            subway_activity = 0.1 * subway_multiplier\n",
    "        else:  # midday\n",
    "            bike_activity = 0.7 * bike_multiplier  # bikes popular midday\n",
    "            subway_activity = 0.5 * subway_multiplier\n",
    "        \n",
    "        frame = {\n",
    "            'timestamp': timestamp.isoformat(),\n",
    "            'day': int(day),  # convert to native int\n",
    "            'hour': int(hour),  # convert to native int\n",
    "            'weather': {\n",
    "                'temp': float(day_weather['temp_f']) if day_weather is not None else 50.0,\n",
    "                'raining': bool(float(day_weather['precipitation']) > 0.1) if day_weather is not None else False,  # convert to bool\n",
    "                'wind': float(day_weather['wind_speed']) if day_weather is not None else 5.0\n",
    "            },\n",
    "            'subway_activity': float(subway_activity),  # ensure float\n",
    "            'bike_activity': float(bike_activity),  # ensure float\n",
    "            'is_rush_hour': bool(hour in [7, 8, 9, 17, 18, 19]),  # explicit bool\n",
    "            'is_weekend': bool(day >= 5)  # explicit bool\n",
    "        }\n",
    "        animation_data.append(frame)\n",
    "\n",
    "print(f\"created {len(animation_data)} hourly frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11b9f6eb-50fb-4853-9171-10f9a4c3abf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7. EXPORTING DATA\n",
      "----------------------------------------\n",
      "saved subway data: 428 stations\n",
      "saved citibike data: 2240 stations\n",
      "saved connections: 3556 links\n",
      "saved animation timeline: 168 frames\n",
      "\n",
      "============================================================\n",
      "DATA PREPARATION COMPLETE!\n",
      "ready to build the visualization\n",
      "output directory: ../visualizations/above_below_data\n"
     ]
    }
   ],
   "source": [
    "#Export All Data for Visualization\n",
    "print(\"\\n7. EXPORTING DATA\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 1. subway stations with hourly patterns\n",
    "subway_export = {\n",
    "    'stations': [],\n",
    "    'hourly_patterns': {}\n",
    "}\n",
    "\n",
    "for station in subway_stations['station_complex'].unique():\n",
    "    station_data = subway_df[subway_df['station_complex'] == station].iloc[0]\n",
    "    \n",
    "    # get hourly pattern\n",
    "    hourly = subway_hourly[subway_hourly['station_complex'] == station].groupby('hour')['ridership'].mean()\n",
    "    \n",
    "    # check if it's an art station\n",
    "    is_art_station = station in art_matches['station_complex'].values\n",
    "    \n",
    "    subway_export['stations'].append({\n",
    "        'name': str(station),  # ensure string\n",
    "        'lat': float(station_data['latitude']),  # ensure float\n",
    "        'lon': float(station_data['longitude']),  # ensure float\n",
    "        'is_portal': bool(is_art_station),  # ensure bool\n",
    "        'art_count': int(len(art_matches[art_matches['station_complex'] == station])) if is_art_station else 0  # ensure int\n",
    "    })\n",
    "    \n",
    "    # convert hourly pattern to list of floats\n",
    "    hourly_list = [float(x) for x in hourly.tolist()] if len(hourly) > 0 else [0.0] * 24\n",
    "    subway_export['hourly_patterns'][str(station)] = hourly_list\n",
    "\n",
    "# 2. citibike stations\n",
    "citibike_export = {\n",
    "    'stations': []\n",
    "}\n",
    "\n",
    "for _, station in citibike_df.iterrows():\n",
    "    citibike_export['stations'].append({\n",
    "        'id': str(station['station_id']),  # ensure string\n",
    "        'name': str(station['name']),  # ensure string\n",
    "        'lat': float(station['lat']),  # ensure float\n",
    "        'lon': float(station['lon']),  # ensure float\n",
    "        'capacity': int(station['capacity']) if pd.notna(station['capacity']) else 20  # ensure int\n",
    "    })\n",
    "\n",
    "# 3. connections between layers - ensure all are native types\n",
    "connections_export = []\n",
    "for _, conn in connections_df.iterrows():\n",
    "    connections_export.append({\n",
    "        'subway_station': str(conn['subway_station']),\n",
    "        'bike_station_id': str(conn['bike_station_id']),\n",
    "        'bike_station_name': str(conn['bike_station_name']),\n",
    "        'distance': float(conn['distance'])\n",
    "    })\n",
    "\n",
    "# 4. save everything\n",
    "with open(output_dir / 'subway_data.json', 'w') as f:\n",
    "    json.dump(subway_export, f)\n",
    "    print(f\"saved subway data: {len(subway_export['stations'])} stations\")\n",
    "\n",
    "with open(output_dir / 'citibike_data.json', 'w') as f:\n",
    "    json.dump(citibike_export, f)\n",
    "    print(f\"saved citibike data: {len(citibike_export['stations'])} stations\")\n",
    "\n",
    "with open(output_dir / 'connections.json', 'w') as f:\n",
    "    json.dump(connections_export, f)\n",
    "    print(f\"saved connections: {len(connections_export)} links\")\n",
    "\n",
    "with open(output_dir / 'animation_timeline.json', 'w') as f:\n",
    "    json.dump(animation_data, f)\n",
    "    print(f\"saved animation timeline: {len(animation_data)} frames\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATA PREPARATION COMPLETE!\")\n",
    "print(\"ready to build the visualization\")\n",
    "print(f\"output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6ca5cb-c311-4f27-b649-a3c893a8e254",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
